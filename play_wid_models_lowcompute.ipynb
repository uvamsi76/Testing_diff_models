{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Testing_diff_models/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline , AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import tokenize_hfmodel_inputs\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_compute=['mistralai/Mistral-7B-v0.3','google/gemma-2b','microsoft/phi-2','meta-llama/Llama-2-7b-hf','teknium/OpenHermes-2.5-Mistral-7B','meta-llama/Llama-3.1-8B','microsoft/Phi-3-mini-128k-instruct']\n",
    "instruct_counter_part=['mistralai/Mistral-7B-Instruct-v0.3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=low_compute[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [02:12<00:00, 66.36s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/Testing_diff_models/utils.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens=torch.tensor(tokens).view(1,tokens.shape[0])\n"
     ]
    }
   ],
   "source": [
    "sample_text=\"How to mak a sandwich\"\n",
    "\n",
    "\n",
    "tokens,tokeniser=tokenize_hfmodel_inputs(sample_text,model_name,'cpu')\n",
    "x=tokens.to(device)\n",
    "for _ in range(50):\n",
    "    ops=model(input_ids=x)\n",
    "    logits=ops.logits\n",
    "    probs=F.softmax(logits[:,-1,:],dim=-1)\n",
    "\n",
    "    next_token=torch.multinomial(probs.view(probs.shape[1]),1)\n",
    "\n",
    "    x=torch.cat([x,next_token.view(1,1)],dim=-1)\n",
    "op=''\n",
    "for i in x.tolist():\n",
    "    op=op+tokeniser.decode(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to mak a sandwich). Toast both slices of bread. Spread mayonnaise and mustard on one slice of bread. Layer turkey, cheese, and lettuce on top. Place the other slice of bread on the top and voila! You have a delicious sandwich ready to be\n"
     ]
    }
   ],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline , AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import tokenize_hfmodel_inputs\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokeniser=AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnEos(StoppingCriteria):\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Check if the last generated token is the EOS token\n",
    "        eos_token_id = 50256  # Change this based on your model's tokenizer\n",
    "        return input_ids[0, -1] == eos_token_id\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnEos()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "write a frontend code to an ecommerce application in react.\n",
      "model\n",
      "```javascript\n",
      "import React, { useState } from \"react\";\n",
      "import \"./App.css\";\n",
      "\n",
      "const App = () => {\n",
      "  const [products, setProducts] = useState([]);\n",
      "\n",
      "  const handleAddProduct = (product) => {\n",
      "    setProducts([...products, product]);\n",
      "  };\n",
      "\n",
      "  return (\n",
      "    <div className=\"app\">\n",
      "      <h1>Ecommerce App</h1>\n",
      "      <input type=\"text\" placeholder=\"Enter product name\" />\n",
      "      <input type=\"number\" placeholder=\"Enter product price\" />\n",
      "      <button onClick={() => handleAddProduct({ name: \"\", price: 0 })}>\n",
      "        Add Product\n",
      "      </button>\n",
      "      <ul>\n",
      "        {products.map((product) => (\n",
      "          <li key={product.id}>{product.name} - ${product.price}</li>\n",
      "        ))}\n",
      "      </ul>\n",
      "    </div>\n",
      "  );\n",
      "};\n",
      "\n",
      "export default App;\n",
      "```\n",
      "\n",
      "**CSS:**\n",
      "\n",
      "```css\n",
      ".app {\n",
      "  display: flex;\n",
      "  flex-wrap: wrap;\n",
      "  margin: 0 auto;\n",
      "  padding: 20px;\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The `useState` hook is used to manage the state of the products.\n",
      "* The `handleAddProduct` function is called when the \"Add Product\" button is clicked. It adds a new product object to the `products` state.\n",
      "* The `products` state is rendered as a list of products.\n",
      "* The `className` attribute is used to style the app container.\n",
      "\n",
      "**How to use:**\n",
      "\n",
      "1. Run the code.\n",
      "2. Enter a product name and price in the input fields.\n",
      "3. Click the \"Add Product\" button.\n",
      "4. The new product will be added to the list.\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* You can customize the `App` component to add other elements, such as a product description or image.\n",
      "* You can also add error handling and validation to improve the user experience.\n"
     ]
    }
   ],
   "source": [
    "sample_text=\"write a frontend code to an ecommerce application in react.\"\n",
    "messages = [{\"role\": \"user\", \"content\": sample_text}]\n",
    "prompt = tokeniser.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "input_ids = tokeniser(prompt, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(input_ids.input_ids,max_length=500,do_sample=True,temperature=0.7,top_p=0.9)\n",
    "response = tokeniser.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "messages.append({\"role\": \"user\", \"content\": \"But you did not write c=a+b anywhere\"})\n",
    "\n",
    "prompt = tokeniser.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "input_ids = tokeniser(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output_ids = model.generate(input_ids.input_ids, max_length=1200, do_sample=True, top_p=0.9)\n",
    "response = tokeniser.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=\"write a c program to print first 5 numbers of fibonacci series.\"\n",
    "model=AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokeniser=AutoTokenizer.from_pretrained(model_name)\n",
    "messages = [{\"role\": \"user\", \"content\": sample_text}]\n",
    "prompt = tokeniser.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "input_ids = tokeniser(prompt, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(input_ids.input_ids,max_length=200,do_sample=True,temperature=0.7,top_p=0.9)\n",
    "response = tokeniser.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
