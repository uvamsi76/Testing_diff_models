{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline , AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import tokenize_hfmodel_inputs\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_coding_models=['meta-llama/CodeLlama-7b-Instruct-hf','bigcode/starcoder2-7b','microsoft/Phi-3-mini-128k-instruct']\n",
    "med_coding_models=['codellama/CodeLlama-13b-hf','bigcode/starcoder2-15b']\n",
    "high_coding_models=['mistralai/Codestral-22B-v0.1','codellama/CodeLlama-34b-Instruct-hf','deepseek-ai/deepseek-coder-33b-instruct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=low_coding_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=\"How can we design linkedlist in python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokens,tokeniser=tokenize_hfmodel_inputs(sample_text,model_name,device)\n",
    "x=tokens.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> def fibanocci(a,b):\n",
      "    if a == 0 or b == 0:\n",
      "        return 0\n",
      "    elif a == 1 or b == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibanocci(a-1) + fibanocci(a-2)\n",
      "#print(fibanocci(4, 6))\n",
      "\n",
      "fizz_buzz = lambda x,n: ['fizz' if i%3 ==0 else ' ' and 'buzz' if i%5== 0 else ' ' for i in range(1,x)]\n",
      "#print(fizz_buzz(16,5))\n",
      "\n",
      "#from itertools import accumulate\n",
      "#print(list(accumulate([])))\n",
      "\n",
      "my_dict = {}\n",
      "my_dict = dict(sorted(my_dict.items(),key = lambda x: x[1]))\n",
      "\n",
      "#my_ls = [1,2,5,3,4]\n",
      "#upper_bound = 100000\n",
      "#y = range(0, upper_bound, 50000)\n",
      "#chunks = (\n"
     ]
    }
   ],
   "source": [
    "for _ in range(200):\n",
    "    ops=model(input_ids=x)\n",
    "    logits=ops.logits\n",
    "    probs=F.softmax(logits[:,-1,:],dim=-1)\n",
    "\n",
    "    next_token=torch.multinomial(probs.view(probs.shape[1]),1)\n",
    "\n",
    "    x=torch.cat([x,next_token.view(1,1)],dim=-1)\n",
    "op=''\n",
    "for i in x.tolist():\n",
    "    op=op+tokeniser.decode(i)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can we design linkedlist in python\n",
      "\n",
      "How can we design linkedlist in python\n",
      "\n",
      "Comment: What do you mean by \"design\"?\n",
      "\n",
      "Comment: http://www.python-course.eu/linked_lists.php\n",
      "\n",
      "Answer: \\begin{code}\n",
      "class Node:\n",
      "    def __init__(self, data):\n",
      "        self.data = data\n",
      "        self.next = None\n",
      "\n",
      "class LinkedList:\n",
      "    def __init__(self):\n",
      "        self.head = None\n",
      "\n",
      "    def append(self, data):\n",
      "        if self.head is None:\n",
      "            self.head = Node(data)\n",
      "            return\n",
      "\n",
      "        node = self.head\n",
      "        while node.next is not None:\n",
      "            node = node.next\n",
      "\n",
      "        node.next = Node(data)\n",
      "\n",
      "    def print(self):\n",
      "        if self.head is None:\n",
      "            return\n",
      "\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "input_ids = tokeniser(sample_text, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(input_ids.input_ids,max_length=200,do_sample=True,temperature=0.7,top_p=0.9)\n",
    "response = tokeniser.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=\"write a c program to print first 5 numbers of fibonacci series.\"\n",
    "model=AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokeniser=AutoTokenizer.from_pretrained(model)\n",
    "messages = [{\"role\": \"user\", \"content\": sample_text}]\n",
    "prompt = tokeniser.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "input_ids = tokeniser(prompt, return_tensors=\"pt\")\n",
    "output_ids = model.generate(input_ids.input_ids,max_length=200,do_sample=True,temperature=0.7,top_p=0.9)\n",
    "response = tokeniser.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "messages.append({\"role\": \"user\", \"content\": \"But you did not write c=a+b anywhere\"})\n",
    "\n",
    "prompt = tokeniser.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "input_ids = tokeniser(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output_ids = model.generate(input_ids.input_ids, max_length=1200, do_sample=True, top_p=0.9)\n",
    "response = tokeniser.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
