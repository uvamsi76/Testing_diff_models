{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vamsi/Desktop/projects/AI projects/Testing_diff_models/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline , AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import tokenize_hfmodel_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot = pipeline(\"text-generation\", \n",
    "#                    model=\"google/gemma-2b\",\n",
    "#                    torch_dtype=torch.float16,\n",
    "#                    device_map=\"auto\",\n",
    "#                    )\n",
    "# response = chatbot(\"sample_text\", max_new_tokens=50)\n",
    "# print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_compute=['mistralai/Mistral-7B-v0.3','google/gemma-2b','microsoft/phi-2','meta-llama/Llama-2-7b-hf','teknium/OpenHermes-2.5-Mistral-7B','meta-llama/Llama-3.1-8B','microsoft/Phi-3-mini-128k-instruct']\n",
    "instruct_counter_part=['mistralai/Mistral-7B-Instruct-v0.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_compute=['mistralai/Mixtral-8x7B-v0.1','meta-llama/Llama-2-13b-hf','CohereForAI/c4ai-command-r-plus-08-2024','']\n",
    "med_instruct_counter_part=['mistralai/Mixtral-8x7B-Instruct-v0.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_compute=['tiiuae/falcon-180B',]\n",
    "large_instruct_counter_part=['tiiuae/falcon-180B-chat','meta-llama/Llama-3.3-70B-Instruct','meta-llama/Llama-2-70b-chat-hf','mistralai/Mistral-Large-Instruct-2411']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_coding_models=['meta-llama/CodeLlama-7b-Instruct-hf','bigcode/starcoder2-7b','microsoft/Phi-3-mini-128k-instruct']\n",
    "med_coding_models=['codellama/CodeLlama-13b-hf','bigcode/starcoder2-15b']\n",
    "high_coding_models=['mistralai/Codestral-22B-v0.1','codellama/CodeLlama-34b-Instruct-hf','deepseek-ai/deepseek-coder-33b-instruct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"google/gemma-2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=\"How to mak a sandwich\"\n",
    "# tokeniser=Autotokeniser.from_pretrained(\"google/gemma-2b\")\n",
    "# tokens=torch.tensor(tokeniser.encode(sample_text))\n",
    "# tokens=torch.tensor(tokens).view(1,tokens.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vamsi/Desktop/projects/AI projects/Testing_diff_models/utils.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens=torch.tensor(tokens).view(1,tokens.shape[0])\n"
     ]
    }
   ],
   "source": [
    "tokens,tokeniser=tokenize_hfmodel_inputs(sample_text,model_name,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#             {\"role\": \"user\", \"content\": sample_text}\n",
    "#         ]\n",
    "# prompt = tokeniser.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# input_ids = tokeniser(prompt, return_tensors=\"pt\")\n",
    "# tkns=input_ids.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tokens\n",
    "for _ in range(50):\n",
    "    ops=model(input_ids=x)\n",
    "    logits=ops.logits\n",
    "    probs=F.softmax(logits[:,-1,:],dim=-1)\n",
    "\n",
    "    next_token=torch.multinomial(probs.view(probs.shape[1]),1)\n",
    "\n",
    "    x=torch.cat([x,next_token.view(1,1)],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' chung'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.decode(next_token.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "op=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x.tolist():\n",
    "    op=op+tokeniser.decode(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "How to mak a sandwich<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Followers Clause\n",
      " contribuvendo á my fb\n",
      " Andrew August 31, 2019\n",
      " Proyect Storey of a since 2019\n",
      " Trendsetouts\n",
      " Townhouse whos konyhok\n",
      " konkeyấp chung\n"
     ]
    }
   ],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops=model.generate(tokens,max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=\"write a c program to print first 5 numbers of fibonacci series.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vamsi/Desktop/projects/AI projects/Testing_diff_models/utils.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens=torch.tensor(tokens).view(1,tokens.shape[0])\n"
     ]
    }
   ],
   "source": [
    "tokens,tokeniser=tokenize_hfmodel_inputs(sample_text,model_name,'cpu',is_chat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            {\"role\": \"user\", \"content\": sample_text}\n",
    "        ]\n",
    "prompt = tokeniser.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = tokeniser(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_ids.input_ids,max_length=200,do_sample=True,temperature=0.7,top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "write a c program to print first 5 numbers of fibonacci series.\n",
      "model\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "int main()\n",
      "{\n",
      "    int a = 0, b = 1, c;\n",
      "\n",
      "    for (int i = 1; i <= 5; i++)\n",
      "    {\n",
      "        printf(\"%d \", a);\n",
      "        a = b;\n",
      "        b = c;\n",
      "    }\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "0 1 1 2 3\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The program uses three variables, `a`, `b`, and `c`, to represent the three consecutive numbers in the Fibonacci sequence.\n",
      "* The `for` loop iterates from `i = 1` to `i <= 5`.\n",
      "* Inside the loop, it swaps the values of `a` and `b`\n"
     ]
    }
   ],
   "source": [
    "response = tokeniser.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "write a c program to print first 5 numbers of fibonacci series.\n",
      "model\n",
      "user\n",
      "write a c program to print first 5 numbers of fibonacci series.\n",
      "model\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "int main()\n",
      "{\n",
      "    int a = 0, b = 1, c;\n",
      "\n",
      "    for (int i = 1; i <= 5; i++)\n",
      "    {\n",
      "        printf(\"%d \", a);\n",
      "        a = b;\n",
      "        b = c;\n",
      "    }\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "0 1 1 2 3\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The program uses three variables, `a`, `b`, and `c`, to represent the three consecutive numbers in the Fibonacci sequence.\n",
      "* The `for` loop iterates from `i = 1` to `i <= 5`.\n",
      "* Inside the loop, it swaps the values of `a` and `b`\n",
      "user\n",
      "But this is wrong answer\n",
      "model\n",
      "user\n",
      "write a c program to print first 5 numbers of fibonacci series.\n",
      "model\n",
      "user\n",
      "write a c program to print first 5 numbers of fibonacci series.\n",
      "model\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "int main()\n",
      "{\n",
      "    int a = 0, b = 1, c;\n",
      "\n",
      "    for (int i = 1; i <= 5; i++)\n",
      "    {\n",
      "        printf(\"%d \", a);\n",
      "        a = b;\n",
      "        b = c;\n",
      "    }\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "0 1 1 2 3\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The program uses three variables, `a`, `b`, and `c`, to represent the three consecutive numbers in the Fibonacci sequence.\n",
      "* The `for` loop iterates from `i = 1` to `i <= 5`.\n",
      "* Inside the loop, it swaps the values of `a` and `b`\n",
      "user\n",
      "But this is wrong answer\n",
      "model\n",
      "The issue with this code is that it swaps the values of `a` and `b` only once, at the beginning of the loop. The `a` and `b` variables are then assigned the values of the previous two numbers in the sequence, resulting in the output you've described.\n",
      "\n",
      "To print the first 5 numbers of the Fibonacci sequence, we can modify the code to update the variables correctly.\n",
      "\n",
      "**Modified Code:**\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "int main()\n",
      "{\n",
      "    int a = 0, b = 1, c;\n",
      "\n",
      "    for (int i = 1; i <= 5; i++)\n",
      "    {\n",
      "        printf(\"%d \", a);\n",
      "        c = a + b;\n",
      "        a = b;\n",
      "        b = c;\n",
      "    }\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "0 1 1 2 3\n",
      "```\n",
      "\n",
      "**Explanation of Changes:**\n",
      "\n",
      "* The new `c` variable is calculated by adding the previous values of `a` and `b`.\n",
      "* This ensures that the values of `a` and `b` are swapped correctly, resulting in the desired output.\n",
      "user\n",
      "But you did not write c=a+b anywhere\n",
      "model\n",
      "Sure, here's the corrected code you requested:\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "int main()\n",
      "{\n",
      "    int a = 0, b = 1, c;\n",
      "\n",
      "    for (int i = 1; i <= 5; i++)\n",
      "    {\n",
      "        printf(\"%d \", a);\n",
      "        c = a + b;\n",
      "        a = b;\n",
      "        b = c;\n",
      "    }\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```\n",
      "0 1 1 2 3\n",
      "```\n",
      "\n",
      "With this corrected code, the program will print the first 5 numbers of the Fibonacci sequence.\n"
     ]
    }
   ],
   "source": [
    "# User asks first question\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})  # Save model response\n",
    "messages.append({\"role\": \"user\", \"content\": \"But you did not write c=a+b anywhere\"})  # New user query\n",
    "\n",
    "# Reformat input\n",
    "prompt = tokeniser.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "input_ids = tokeniser(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate again\n",
    "output_ids = model.generate(input_ids.input_ids, max_length=1200, do_sample=True, top_p=0.9)\n",
    "response = tokeniser.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
